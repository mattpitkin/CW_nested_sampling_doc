\section{Overview}

There are a variety of sources of \gws that are potentially observable using the current ground-based \gw detectors \citep{2015CQGra..32g4001L,2015CQGra..32b4001A}.
These include transient signals, such as those already observed from the merger of binary black hole systems \citep{2016PhRvL.116f1102A,2016PhRvL.116x1103A}, and also
continuous quasi-monochromatic signals, such as expected from assymetrically deformed rotating neutron stars \citep[see, e.g., Section~III of][]{2004PhRvD..69h2004A}.
Here we will describe a code used in searches for continuous \gw signals, in particular signals from sources that are seen as pulsars via electromagnetic observations.

One of the primary methods used in searches for \gws from known pulsars is based on two stages: a heterodyne
stage that pre-processes the calibrated \gw detector data by removing a signal's expected intrinsic phase
evolution (based on the observed solution from electromagnetic observations), whilst also low-pass filtering and
down-sampling (via averaging) the heterodyned data \citep{2005PhRvD..72j2002D}; and, a parameter estimation stage that takes the
processed data and uses it to estimate posterior probability distributions of the unknown signal parameters
\citep[e.g., using a Markov chain Monte Carlo (MCMC)][]{2010ApJ...713..671A}. This method has been variously
called the {\it Time Domain Method}, the {\it Heterodyne Method}, the {\it Bayesian Method}, the {\it Glasgow
Method}, or some combination of these names. Up to now this method has only been used to set upper limits
on signal amplitudes, but has included no explicit criteria for deciding on signal detection/significance. A further
limitation has been that the MCMC method used was inefficient\footnote{MCMC methods are not intrinsically inefficient for
searches such as ours, but the particular implementation we had used was not specifically designed to be efficient at sampling the
given posterior volumes.} and required a lot of tuning. It also
did not straightforwardly have the ability to perform a search on the data over small ranges in the phase
evolution parameters, which is required if the \gw signal does not well match the known pulsar's phase evolution. A nested sampling
algorithm \citep{Skilling:2006}, in particular a method based on that described in \citet{Veitch:2010}, has been provided within
the \lalinf software library \citep{2015PhRvD..91d2003V} in \citet{lalsuite}. This has been used for parameter estimation and model
selection for both modelled transient sources, such as the observed black hole mergers \citep{2016PhRvL.116x1102A}, and unmodelled transient signal
searches \citep{2015arXiv151105955L}. Due to the above reasons, and to make use of the existing and well used library functions, the code has been
re-written to use the available nested sampling algorithm. This allows us to evaluate the {\it evidence} or {\it
marginal likelihood} for the signal model and compare it to other model evidences (i.e.\ the data is consistent with being just
Gaussian noise), whilst also giving posterior probability distributions for the signal parameters.

This code is called \lppenf (or \lppen for short through the rest of this document). For more detailed descriptions of how the algorithm works we refer
to \citet{Veitch:2010} and \citep{2015PhRvD..91d2003V}, whilst here we will provide some information on the specific proposal
distributions that can be used within the algorithm. This method has previously been briefly described in \citet{2012JPhCS.363a2041P}.
The previous code, called \lppef (or \lppe for short), used in, e.g., \citet{2010ApJ...713..671A}, could perform posterior evaluation
using two methods: an MCMC method, or, by gridding up the parameter space and explicitly evaluating the posterior at each
grid point. In \S\ref{sec:codeeval} we show comparisons of the various methods mainly to check \lppen for consistency.

The code can also be used to perform parameter estimation and model selection for signals for any generic
metric theory of gravity, however its use for this is discussed in more detail in a separate paper \citep{MaxCWpolariations}. When
searching over parameters that vary the phase evolution there is potential to speed up the analysis through
efficient likelihood evaluation via the {\it reduced order quadrature} method \citep[see, e.g.,][]{2014PhRvX...4c1006F,2015PhRvL.114g1104C},
but again that will be discussed in more detail in a future paper.

\subsection{Background knowledge}\label{sec:general}

The code calculates the Bayesian evidence, or {\it marginal likelihood}, for a particular signal model under
a set of assumptions. The evidence, $\mathcal{Z}$, for a given model, $M$, defined by a set of parameters, $\vec{\theta}$, is given by
\begin{equation}\label{eq:evidence}
\mathcal{Z}_M = p(d|M,I) = \int^{\vec{\theta}} p(d|\vec{\theta},M,I)p(\vec{\theta}|M,I) {\rm d}\vec{\theta},
\end{equation}
where $p(d|\vec{\theta},M,I)$ is the likelihood function for the data $d$ given the
model and its set of defining parameters, $p(\vec{\theta}|M,I)$ is the prior probability distribution on
$\vec{\theta}$, and $I$ represents any additional prior assumptions. During nested sampling this integral is evaluated by transforming it into the one dimensional
sum
\begin{equation}\label{eq:nestedsampev}
\mathcal{Z}_M = \sum_i^N p(d|\vec{\theta}_i,M,I) w_i,
\end{equation}
where $w_i = p(\vec{\theta}_i|M,I) \Delta\vec{\theta}_i$ is the ``prior weight''
(i.e.\ the fraction of the prior occupied by point $i$).

By default the signal model evidence is compared to the evidence that the data consists of only Gaussian noise to form the odds for the two models
\begin{align}\label{eq:oddsratio}
\mathcal{O} &= \frac{p(d|M,I)}{p(d|\text{noise},I)}\frac{p(M|I)}{p(\text{noise}|I)} \nonumber \\
&= \frac{\mathcal{Z}_M}{\mathcal{Z}_{\text{noise}}},
\end{align}
where on the right hand side we have explicitly set the prior odds for the two models to
$p(M|I)/p(\text{noise}|I) = 1$.

Other than this evidence value and odds, the code also returns the samples accumulated during
the nested sampling process. These samples are not drawn from the posterior probability distribution as in
an MCMC method, but they can be resampled to generate a subset of samples that are drawn from the posterior
distribution. This resampling \citep[performed using the {\tt lalapps\_nest2pos} {\tt python} script within
{\tt lalapps}][]{lalsuite} uses the
value of $L_i w_i$ for each point, normalised by $(L_iw_i)_{\rm max}$, to give values proportional to the
posterior probability, and then accepts a point with a probability given by its value, i.e.\ a point is
accepted with the probability
\begin{equation}\label{eq:postaccept}
P_{\text{accept}} = \frac{L_i w_i}{(L_iw_i)_{\rm max}}.
\end{equation}

In our case the data $d$ in equation~\ref{eq:oddsratio} is the heterodyned, low-pass filtered and down-sampled data
from a detector, or set of detectors, each giving a single data stream or two data streams depending on
whether the heterodyne was performed at one or both potential signal frequencies near the rotation frequency
and/or twice the rotation frequency. Here we will use $\mathbf{B}$ to represent a vector of these heterodyned
data values \citep{2005PhRvD..72j2002D}, for which a single time sample is often referred to using the jargon term $B_k \equiv B(t_k)$ (``{\it B of k}''), although we will
not consistently use $k$ as the time index for the data. Throughout this document if we refer to ``heterodyned'' data we mean
data that has been processed, or simulated as if processed, by this method.
